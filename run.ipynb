{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "1a16d3a2b4b78bd0c8a054524d667d1c",
     "grade": false,
     "grade_id": "cell-3a093c227c1a8513",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from utils import *\n",
    "from tqdm import tqdm\n",
    "\n",
    "import os \n",
    "import shutil\n",
    "\n",
    "import wandb \n",
    "import warnings\n",
    "\n",
    "import gym \n",
    "from gym.wrappers.monitoring.video_recorder import VideoRecorder\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "os.environ[\"WANDB_NOTEBOOK_NAME\"] = \"run.ipynb\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment_parameters = {\n",
    "    \"num_runs\" : 10,\n",
    "    \"num_episodes\" : 1000,\n",
    "    # OpenAI Gym environments allow for a timestep limit timeout, causing episodes to end after some number of timesteps\n",
    "    \"timeout\" : 500\n",
    "}\n",
    "\n",
    "environment_parameters = {}\n",
    "current_env = LunarLanderEnvironment\n",
    "\n",
    "agent_parameters = {\n",
    "    'network_config': {\n",
    "        'state_dim': 8,\n",
    "        'num_hidden_units': 256,\n",
    "        'num_actions': 4\n",
    "    },\n",
    "    'optimizer_config': {\n",
    "        'step_size': 1e-3,\n",
    "        'beta_m': 0.9, \n",
    "        'beta_v': 0.999,\n",
    "        'epsilon': 1e-8\n",
    "    },\n",
    "    'replay_buffer_size': 50000,\n",
    "    'minibatch_sz': 8,\n",
    "    'num_replay_updates_per_step': 4,\n",
    "    'gamma': 0.99,\n",
    "    'tau': 0.001\n",
    "}\n",
    "\n",
    "current_agent = Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "e192cd7f474ff57861f6f8a3e3ab188c",
     "grade": false,
     "grade_id": "cell-0defecc3f69370dc",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def run_experiment(environment, agent, environment_parameters, agent_parameters, experiment_parameters):\n",
    "    rl_glue = RLGlue(environment, agent)\n",
    "        \n",
    "    # save sum of reward at the end of each episode\n",
    "    agent_sum_reward = np.zeros((experiment_parameters[\"num_runs\"], \n",
    "                                 experiment_parameters[\"num_episodes\"]))\n",
    "\n",
    "    env_info = {}\n",
    "    agent_info = agent_parameters\n",
    "\n",
    "    for run in range(1, experiment_parameters[\"num_runs\"]+1):\n",
    "        wandb.init(project=\"RL_Lunar_Lander\")\n",
    "        agent_info[\"seed\"] = run\n",
    "        agent_info[\"network_config\"][\"seed\"] = run\n",
    "        env_info[\"seed\"] = run\n",
    "\n",
    "        rl_glue.rl_init(agent_info, env_info)\n",
    "        \n",
    "        for episode in tqdm(range(1, experiment_parameters[\"num_episodes\"]+1)):\n",
    "\n",
    "            # run episode\n",
    "            rl_glue.rl_episode(experiment_parameters[\"timeout\"])\n",
    "            \n",
    "            # get cumulative reward\n",
    "            episode_reward = rl_glue.rl_agent_message(\"get_sum_reward\")\n",
    "            agent_sum_reward[run-1, episode-1] = episode_reward\n",
    "            wandb.log({\"episode_reward\": episode_reward})\n",
    "\n",
    "    save_name = \"{}\".format(rl_glue.agent.name)\n",
    "    if not os.path.exists('results'):\n",
    "        os.makedirs('results')\n",
    "    np.save(\"results/sum_reward_{}\".format(save_name), agent_sum_reward)\n",
    "    shutil.make_archive('results', 'zip', 'results')\n",
    "\n",
    "    return rl_glue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rl_glue = run_experiment(current_env, current_agent, environment_parameters, agent_parameters, experiment_parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(\"LunarLander-v2\")\n",
    "\n",
    "# get trained agent\n",
    "agent = rl_glue.agent\n",
    "\n",
    "for i in range(10):\n",
    "    video = VideoRecorder(env, f\"logs/video_{i}.mp4\")\n",
    "    observation = env.reset()\n",
    "\n",
    "    for j in range(1000):\n",
    "        env.render()\n",
    "        video.capture_frame()\n",
    "        action = agent.policy(observation)\n",
    "        observation, reward, done, info = env.step(action)\n",
    "\n",
    "        if done:\n",
    "            break\n",
    "\n",
    "    video.close()\n",
    "\n",
    "env.close()"
   ]
  }
 ],
 "metadata": {
  "coursera": {
   "course_slug": "complete-reinforcement-learning-system",
   "graded_item_id": "8dMlx",
   "launcher_item_id": "4O5gG"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
