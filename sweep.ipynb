{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from utils import *\n",
    "from tqdm import tqdm\n",
    "\n",
    "import os \n",
    "import shutil\n",
    "\n",
    "import wandb \n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "os.environ[\"WANDB_NOTEBOOK_NAME\"] = \"sweep.ipynb\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment_parameters = {\n",
    "    \"num_runs\" : 1,\n",
    "    \"num_episodes\" : 1000,\n",
    "    # OpenAI Gym environments allow for a timestep limit timeout, causing episodes to end after some number of timesteps\n",
    "    \"timeout\" : 500\n",
    "}\n",
    "\n",
    "environment_parameters = {}\n",
    "\n",
    "current_env = LunarLanderEnvironment\n",
    "current_agent = Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_experiment(environment, agent, environment_parameters, agent_parameters, experiment_parameters):\n",
    "    rl_glue = RLGlue(environment, agent)\n",
    "        \n",
    "    # save sum of reward at the end of each episode\n",
    "    agent_sum_reward = np.zeros((experiment_parameters[\"num_runs\"], \n",
    "                                 experiment_parameters[\"num_episodes\"]))\n",
    "\n",
    "    env_info = {}\n",
    "    agent_info = agent_parameters\n",
    "\n",
    "    for run in range(1, experiment_parameters[\"num_runs\"]+1):\n",
    "        agent_info[\"seed\"] = run\n",
    "        agent_info[\"network_config\"][\"seed\"] = run\n",
    "        env_info[\"seed\"] = run\n",
    "\n",
    "        rl_glue.rl_init(agent_info, env_info)\n",
    "        \n",
    "        for episode in tqdm(range(1, experiment_parameters[\"num_episodes\"]+1)):\n",
    "\n",
    "            # run episode\n",
    "            rl_glue.rl_episode(experiment_parameters[\"timeout\"])\n",
    "            \n",
    "            # get cumulative reward\n",
    "            episode_reward = rl_glue.rl_agent_message(\"get_sum_reward\")\n",
    "            agent_sum_reward[run-1, episode-1] = episode_reward\n",
    "\n",
    "    average_sum_reward = np.mean(agent_sum_reward, axis=0)\n",
    "    save_name = \"{}\".format(rl_glue.agent.name)\n",
    "    if not os.path.exists('results'):\n",
    "        os.makedirs('results')\n",
    "    np.save(\"results/sum_reward_{}\".format(save_name), agent_sum_reward)\n",
    "    shutil.make_archive('results', 'zip', 'results')\n",
    "\n",
    "    return average_sum_reward[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sweep_config = {\n",
    "    \"method\": \"random\", \n",
    "    \"metric\": {\n",
    "      \"name\": \"sum_reward\",\n",
    "      \"goal\": \"maximize\"   \n",
    "    },\n",
    "    \"parameters\": {\n",
    "        \"num_hidden_units\": {\n",
    "            \"distribution\": \"int_uniform\",\n",
    "            \"min\": 16,\n",
    "            \"max\": 512\n",
    "        },\n",
    "        \"step_size\": {\n",
    "            \"distribution\": \"log_uniform_values\",\n",
    "            \"min\": 1e-4,\n",
    "            \"max\": 1e-2\n",
    "        },\n",
    "        \"tau\": {\n",
    "            \"distribution\": \"log_uniform_values\",\n",
    "            \"min\": 1e-4,\n",
    "            \"max\": 1e-2\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "sweep_id = wandb.sweep(sweep_config, project=\"RL_Lunar_Lander\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train():\n",
    "    config_defaults = {\n",
    "        \"num_hidden_units\": 256,\n",
    "        \"step_size\": 1e-3,\n",
    "        \"tau\": 1e-3,\n",
    "    }\n",
    "\n",
    "    wandb.init(config=config_defaults)  # defaults are over-ridden during the sweep\n",
    "    config = wandb.config\n",
    "\n",
    "    agent_parameters = {\n",
    "        'network_config': {\n",
    "            'state_dim': 8,\n",
    "            'num_hidden_units': config.num_hidden_units,\n",
    "            'num_actions': 4\n",
    "        },\n",
    "        'optimizer_config': {\n",
    "            'step_size': config.step_size,\n",
    "            'beta_m': 0.9, \n",
    "            'beta_v': 0.999,\n",
    "            'epsilon': 1e-8\n",
    "        },\n",
    "        'replay_buffer_size': 50000,\n",
    "        'minibatch_sz': 8,\n",
    "        'num_replay_updates_per_step': 4,\n",
    "        'gamma': 0.99,\n",
    "        'tau': config.tau\n",
    "    }\n",
    "\n",
    "    sum_reward = run_experiment(current_env, current_agent, environment_parameters, agent_parameters, experiment_parameters)\n",
    "    wandb.log({\"sum_reward\": sum_reward})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb.agent(sweep_id, train, count=30)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
